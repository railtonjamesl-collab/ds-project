---
title: "Assessment 0 Documentation"
author: "Billy Ryan"
date: "2025-10-06"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(repos = c(CRAN = "https://cloud.r-project.org"))
```

## Read the data

Before any data analysis can begin, we must first import out data set and required libraries. The data set in question can be found at https://www.kaggle.com/datasets/ealaxi/paysim1. It is quite a large data set so it may take some time to download and load into R.

```{r tidyverse}
install.packages("tidyverse")
install.packages("caret")
```

```{r packages}
pkgs <- c("ggplot2","plyr","corrplot","dplyr","magrittr","RColorBrewer", "data.table")

for(p in pkgs){
    if (!require(p,character.only=TRUE)) install.packages(p)
}
```

Now we load the data

```{r loading dataset}
# R has timeout set to 60 by default, which is likely not enough time to download and import such a big data set hence we increase it here
options(timeout = 1000)

# If not data set is not already downloaded, fetch it
if (!file.exists("PS_20174392719_1491204439457_log.csv")) {
  message("Downloading large dataset, this may take a few minutes.")
  download.file("https://media.githubusercontent.com/media/railtonjamesl-collab/ds-project/refs/heads/main/dataset/PS_20174392719_1491204439457_log.csv", "PS_20174392719_1491204439457_log.csv", mode = "wb")
}

# And now we are able to load the data
dataset <- fread("PS_20174392719_1491204439457_log.csv")
```
Observe the data at hand briefly to make sure there is nothing strange going on i.e. missing variables. 

```{r libraries}
head(dataset)
```
```{r sum}
summary(dataset)
```

```{r dim}
dim(dataset)
```


```{r na check}
sum(is.na(dataset))
```

Everything looks reasonable, with some outliers being visible, specifically in the numerical variables. We will now proceed with some EDA to try and understand the data more astutely.

One of our first questions when looking at the data is how many transactions are fraudulent? 

```{r fraud transactions total}
table(dataset$isFraud)
table(dataset$isFraud)/length(dataset$isFraud)
```
The "0" represents non-fraudulent transactions and "1" fraudulent transactions. Here we see that roughly 99.87% of all transactions are non-fraudulent and 0.13% of transactions are fraudulent. Since we are asked to use visualisations in this assessment, we will produce many different types of plots to better understand the data at hand:

```{r plot non fraud vs fraud}
barplot(table(dataset$isFraud), main="Number of Non-Fraudulent and Fraudulent Transactions", ylab="Count", ylim=c(0,7000000), col = c("skyblue"))
```
Using this plot and the beforehand calculation, we can easily determine that the vast majority of transactions are non-fraudulent. 

It might be of interest to see how the transactions are spaced out by type.

```{r transactions bar plot}
barplot(table(dataset$type),
        main="Number of Transactions in Each Transaction Type",
        xlab="Transaction Type",
        ylab="Count",
        ylim=c(0,2500000),
        col=c("skyblue"))
```
We may also with to visualise this using a pie chart, as it may be easier for comparisons

```{r transactions pie chart}
info <- c(sum(dataset$type=="CASH_OUT"),sum(dataset$type=="CASH_IN"),sum(dataset$type=="DEBIT"),sum(dataset$type=="PAYMENT"),sum(dataset$type=="TRANSFER"))
names <- c("cash_out", "cash_in", "debit", "payment", "transfer")
pct <- round(info/sum(info)*100, digits=2)
names <- paste(names, pct, "%")
pie(info,labels = names, main = "Pie Chart of Transaction Type")
```

This tells us that generally most transactions are either payments and cash related. We can make these graphs more useful by only considering fraudulent transactions, giving us an opportunity to make better inferences.

```{r fraud transactions bar and pie}
table(factor(dataset$type[dataset$isFraud == 1],levels = unique(dataset$type)))
barplot(table(factor(dataset$type[dataset$isFraud == 1],levels = unique(dataset$type))),
        main="Number of Fraudulent Transactions in Each Transaction Type",
        xlab="Transaction Type",
        ylab="Count",
        ylim=c(0,6000),
        col=c("skyblue"))

info <- c(sum(dataset$type[dataset$isFraud==1]=="CASH_OUT"),sum(dataset$type[dataset$isFraud==1]=="TRANSFER"))
names <- c("cash_out", "transfer")
pct <- round((info/sum(info)*100),digits=2)
names <- paste(names, pct, "%")
pie(info,labels = names, main = "Pie Chart of Fraudulent Transaction Type")
```
As we see, the categories of PAYMENT, DEBIT and CASH_IN have no fraudulent transactions, whilst the remaining categories of TRANSFER and CASH_OUT have roughly a 50/50 split of 4000 transactions each that are fraudulent. This is where expert analysis may be of use, as it will allow us to deduce the usefulness of this specific data. 

We will now compute histograms to help us compare transactions and fraudulent transactions.

```{r histogram}
hist(dataset$step,
     main="Number of Transaction against time in hours over 30 days",
     xlab="Time in hour over 30 days",
     ylab="Count",
     ylim=c(0,1500000),
     xlim=c(0,800),
     col=c("skyblue"))

hist(dataset$step[dataset$isFraud==1],
     main="Number of Fraudulent Transactions against time in hours over 30 days",
     xlab="Time in hour over 30 days",
     ylab="Count",
     ylim=c(0,1000),
     xlim=c(0,800),
     col=c("skyblue"))
```
We can see the difference in all transactions versus only fraudulent transactions, that being the fraudulent ones are much more uniformly distributed. We may find it interesting to see when transactions occur depending on the time of day, which we can do easily using the following code.

```{r cain}
dataset$hour <- mod(dataset$step, 24)

hist(dataset$hour,
     main="Number of Transaction against time in hour over 30 days",
     xlab="Time in hour over 30 days",
     ylab="Count",
     ylim=c(0,800000),
     xlim=c(0,25),
     col=c("skyblue"))

hist(dataset$hour[dataset$isFraud==1],
     main="Number of Transaction against time in hour over 30 days",
     xlab="Time in hour over 30 days",
     ylab="Count",
     ylim=c(0,1200),
     xlim=c(0,25),
     col=c("skyblue"))
```
Unsurprisingly, many of the transactions happen between the hours of 08:00 and 23:00, when people are most likely to be awake. Comparing that to fraudulent transactions, we see the same uniform distribution, with a spike at 00:00.

It is also useful to observe correlation between the data

```{r correlation}
numdata <- dataset[,c(1,3,5,6,8,9,10,11)]
corr_mat <- round(cor(numdata, use = "pairwise.complete.obs"), 2)

melted_corr_mat <- as.data.frame(as.table(corr_mat))
names(melted_corr_mat) <- c("Var1","Var2","value")

ggplot(data = melted_corr_mat, aes(x=Var1, y=Var2,fill=value)) + labs(title = "Correlation table of  Variables", fill = "Correlation") + geom_tile() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 10), axis.text.y = element_text(size = 10)) +geom_text(aes(Var2, Var1, label = value),color = "black", size = 4)
```
As we can see, most of the variables do not have a strong correlation between them with the exception of oldbalanceDest and newbalanceDest, which is unsurprising.

We can now move on to using logistic regression on this data set. We will use concepts and code snippets from https://www.r-bloggers.com/2019/11/logistic-regression-in-r-a-classification-technique-to-predict-credit-card-default/ and https://www.kaggle.com/code/mahmoud86/logistic-regression-simulation.


When running this code initially, the vast number of observations made it too computationally expensive to run the logistic regression on the entire data set, hence we have limited the observations so the code can be ran in a timely manner. This will make it so the results will lose some accuracy, but is needed for the computational aspect of this assessment all whilst still allowing us to make valuable inferences.

```{r logistic regression setup}
test_data<-dataset[,c("isFraud","step","amount","oldbalanceOrg","newbalanceOrig","oldbalanceDest","newbalanceDest","isFlaggedFraud")]
test_data$isFraud<-as.factor(test_data$isFraud)
test_data$isFlaggedFraud<-as.factor(test_data$isFlaggedFraud)
set.seed (2)
sample_rows <- sample(nrow(test_data), 600000)
sampled_test_data <- test_data[sample_rows, ]
```

Now that we have our sampled our test data, we can split the data for training and testing, which can be used for evaluation. We will consider a 70/30 split. This means that 70% of our sampled data is used for training and 30% for testing.

```{r logistic regression split}
set.seed(2)
index <- sample(2, nrow(sampled_test_data), replace = TRUE, prob = c(0.7, 0.3))
train <- sampled_test_data[index == 1, ]
test  <- sampled_test_data[index == 2, ]
```

```{r dimension of train and test}
dim(train)
dim(test)
```
We can see the 70/30 split from this quite confidently. We will now create a logistic regression model using the glm command.

```{r glm}
glm.fit <- glm(isFraud ~ ., data = sampled_test_data, family = binomial)
summary(glm.fit)
```
We can use some evaluation techniques to see how accurate our model is, such as the confusion matrix.

```{r evaluation}
glm.probs = predict(glm.fit ,type ="response")
glm.pred=rep("0",1250)
glm.pred[glm.probs >.5]="1"
table(glm.pred,sampled_test_data$isFraud)
```
We can then compute our accuracy using a simple formula.

```{r accuracy}
accuracy <- table(glm.pred, sampled_test_data$isFraud)
sum(diag(accuracy))/sum(accuracy)
```
What we are seeing is that the values on the diagonal of the confusion matrix are "correct" predictions, whilst the off diagonal elements are "incorrect" predictions. We see then that our model has an accuracy of 93%. Since we are using a synthetic data set, this number could be attributed to the fact that the data is inherently "nice" for that purpose. Even so, the logistic regression model does seem to fit our data quite well.